{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf66499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import tweepy\n",
    "import requests\n",
    "from transformers import pipeline\n",
    "import schedule\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc1d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7439528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterNLPModel:\n",
    "    \"\"\"NLP model for analyzing tweets and identifying traffic disruptions with coordinates\"\"\"\n",
    "    \n",
    "    def __init__(self, twitter_bearer_token: str, google_maps_api_key: str = None):\n",
    "        \"\"\"\n",
    "        Initialize the Twitter NLP model\n",
    "        \n",
    "        Args:\n",
    "            twitter_bearer_token: Twitter API Bearer Token\n",
    "            google_maps_api_key: Google Maps API key for geocoding (optional)\n",
    "        \"\"\"\n",
    "        self.bearer_token = twitter_bearer_token\n",
    "        self.google_maps_api_key = google_maps_api_key\n",
    "        self.client = tweepy.Client(bearer_token=twitter_bearer_token)\n",
    "        \n",
    "        # Initialize zero-shot classification pipeline (based on your existing code)\n",
    "        self.classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "        \n",
    "        # Define disruption labels\n",
    "        self.disruption_labels = [\"protest\", \"accident\", \"road closure\", \"traffic jam\", \"construction\", \"emergency\", \"normal\"]\n",
    "        \n",
    "        # Location extraction patterns\n",
    "        self.location_patterns = [\n",
    "            r'\\b(?:at|on|near|in|around)\\s+([A-Z][a-zA-Z\\s]+(?:Road|Street|Ave|Avenue|Blvd|Boulevard|Lane|Drive|Highway|Hwy|Freeway|Bridge|Tunnel))\\b',\n",
    "            r'\\b([A-Z][a-zA-Z\\s]+(?:Road|Street|Ave|Avenue|Blvd|Boulevard|Lane|Drive|Highway|Hwy|Freeway|Bridge|Tunnel))\\b',\n",
    "            r'#([A-Za-z0-9_]+)',\n",
    "            r'\\b([A-Z][a-zA-Z\\s]+(?:Junction|Intersection|Circle|Square|Plaza|Area))\\b'\n",
    "        ]\n",
    "        \n",
    "        # Cache for geocoding results\n",
    "        self.geocode_cache = {}\n",
    "        \n",
    "        # Mock coordinates for demo (Chennai locations)\n",
    "        self.mock_locations = {\n",
    "            'nungambakkam': (13.0569, 80.2412),\n",
    "            'anna nagar': (13.0850, 80.2101),\n",
    "            't nagar': (13.0418, 80.2341),\n",
    "            'adyar': (13.0067, 80.2206),\n",
    "            'velachery': (12.9816, 80.2209),\n",
    "            'tambaram': (12.9249, 80.1000),\n",
    "            'chrompet': (12.9516, 80.1462),\n",
    "            'omr': (12.8956, 80.2267),\n",
    "            'ecr': (12.8270, 80.2420),\n",
    "            'gst road': (12.9165, 80.1315),\n",
    "            'mount road': (13.0569, 80.2412),\n",
    "            'anna salai': (13.0569, 80.2412),\n",
    "            'chennai airport': (12.9941, 80.1709),\n",
    "            'central station': (13.0836, 80.2754),\n",
    "            'egmore': (13.0732, 80.2609),\n",
    "            'guindy': (13.0067, 80.2206),\n",
    "            'porur': (13.0382, 80.1595),\n",
    "            'vadapalani': (13.0524, 80.2123),\n",
    "            'koyambedu': (13.0698, 80.1947),\n",
    "            'kilpauk': (13.0889, 80.2425)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62e51f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(self, text: str) -> str:\n",
    "        \"\"\"Clean and preprocess tweet text\"\"\"\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove mentions but keep hashtags\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7777890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_event(self, text: str) -> Dict:\n",
    "        \"\"\"Classify tweet using zero-shot classification (based on your existing code)\"\"\"\n",
    "        try:\n",
    "            result = self.classifier(text, self.disruption_labels)\n",
    "            return {\n",
    "                \"label\": result[\"labels\"][0],\n",
    "                \"confidence\": result[\"scores\"][0],\n",
    "                \"all_scores\": dict(zip(result[\"labels\"], result[\"scores\"]))\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error classifying tweet: {e}\")\n",
    "            return {\"label\": \"normal\", \"confidence\": 0.0, \"all_scores\": {}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e15e3a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_locations(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract location mentions from tweet text\"\"\"\n",
    "        locations = []\n",
    "        \n",
    "        # Use regex patterns to find locations\n",
    "        for pattern in self.location_patterns:\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            locations.extend(matches)\n",
    "        \n",
    "        # Also look for city area names\n",
    "        text_lower = text.lower()\n",
    "        for location in self.mock_locations.keys():\n",
    "            if location in text_lower:\n",
    "                locations.append(location)\n",
    "        \n",
    "        return list(set(locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e10ff631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geocode_location(self, location: str) -> Optional[Tuple[float, float]]:\n",
    "        \"\"\"Convert location name to coordinates\"\"\"\n",
    "        if location in self.geocode_cache:\n",
    "            return self.geocode_cache[location]\n",
    "        \n",
    "        coords = None\n",
    "        location_lower = location.lower()\n",
    "        \n",
    "        # Check mock locations first\n",
    "        for mock_loc, mock_coords in self.mock_locations.items():\n",
    "            if mock_loc in location_lower or location_lower in mock_loc:\n",
    "                coords = mock_coords\n",
    "                break\n",
    "        \n",
    "        # If not found and Google Maps API is available\n",
    "        if coords is None and self.google_maps_api_key:\n",
    "            try:\n",
    "                url = f\"https://maps.googleapis.com/maps/api/geocode/json\"\n",
    "                params = {\n",
    "                    'address': f\"{location}, Chennai, India\",\n",
    "                    'key': self.google_maps_api_key\n",
    "                }\n",
    "                \n",
    "                response = requests.get(url, params=params)\n",
    "                data = response.json()\n",
    "                \n",
    "                if data['status'] == 'OK' and data['results']:\n",
    "                    location_data = data['results'][0]['geometry']['location']\n",
    "                    coords = (location_data['lat'], location_data['lng'])\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error geocoding with Google Maps: {e}\")\n",
    "        \n",
    "        # Default to Chennai center if not found\n",
    "        if coords is None:\n",
    "            coords = (13.0827, 80.2707)  # Chennai coordinates\n",
    "        \n",
    "        # Cache the result\n",
    "        self.geocode_cache[location] = coords\n",
    "        return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35bd69f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_tweets(self, query: str = \"traffic OR accident OR road closed OR protest\", \n",
    "                    max_results: int = 100) -> List[Dict]:\n",
    "        \"\"\"Fetch tweets from Twitter API\"\"\"\n",
    "        tweets_data = []\n",
    "        \n",
    "        try:\n",
    "            # Fetch tweets\n",
    "            tweets = self.client.search_recent_tweets(\n",
    "                query=f\"{query} -is:retweet lang:en\",\n",
    "                max_results=max_results,\n",
    "                tweet_fields=['created_at', 'geo', 'context_annotations', 'public_metrics']\n",
    "            )\n",
    "            \n",
    "            if not tweets.data:\n",
    "                logger.info(\"No tweets found\")\n",
    "                return tweets_data\n",
    "            \n",
    "            for tweet in tweets.data:\n",
    "                tweets_data.append({\n",
    "                    'id': tweet.id,\n",
    "                    'text': tweet.text,\n",
    "                    'created_at': tweet.created_at,\n",
    "                    'geo': tweet.geo if hasattr(tweet, 'geo') else None\n",
    "                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching tweets: {e}\")\n",
    "            # Return mock tweets for demo\n",
    "            tweets_data = self.get_mock_tweets()\n",
    "        \n",
    "        return tweets_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "775ba0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mock_tweets(self) -> List[Dict]:\n",
    "        \"\"\"Generate mock tweets for demo purposes\"\"\"\n",
    "        mock_tweets = [\n",
    "            {\n",
    "                'id': '1',\n",
    "                'text': 'Heavy traffic jam at Nungambakkam area due to accident. Avoid the route!',\n",
    "                'created_at': datetime.now(),\n",
    "                'geo': None\n",
    "            },\n",
    "            {\n",
    "                'id': '2',\n",
    "                'text': 'Protest happening near Anna Nagar causing road closures #ChennaiTraffic',\n",
    "                'created_at': datetime.now() - timedelta(minutes=15),\n",
    "                'geo': None\n",
    "            },\n",
    "            {\n",
    "                'id': '3',\n",
    "                'text': 'Construction work on OMR causing major delays. Take alternate route',\n",
    "                'created_at': datetime.now() - timedelta(minutes=30),\n",
    "                'geo': None\n",
    "            },\n",
    "            {\n",
    "                'id': '4',\n",
    "                'text': 'Emergency vehicles blocking Mount Road. Traffic moving slowly',\n",
    "                'created_at': datetime.now() - timedelta(minutes=45),\n",
    "                'geo': None\n",
    "            },\n",
    "            {\n",
    "                'id': '5',\n",
    "                'text': 'Water logging at Velachery junction after heavy rain. Roads flooded',\n",
    "                'created_at': datetime.now() - timedelta(minutes=60),\n",
    "                'geo': None\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        return mock_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79446150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tweets(self, query: str = \"traffic OR accident OR road closed OR protest\") -> List[Dict]:\n",
    "        \"\"\"Analyze tweets and return disruption alerts with coordinates\"\"\"\n",
    "        alerts = []\n",
    "        \n",
    "        # Fetch tweets\n",
    "        tweets = self.fetch_tweets(query)\n",
    "        \n",
    "        logger.info(f\"Analyzing {len(tweets)} tweets...\")\n",
    "        \n",
    "        for tweet in tweets:\n",
    "            try:\n",
    "                # Preprocess tweet text\n",
    "                clean_text = self.preprocess_tweet(tweet['text'])\n",
    "                \n",
    "                # Classify the event\n",
    "                classification = self.classify_event(clean_text)\n",
    "                \n",
    "                # Only process if it's not classified as 'normal' and has good confidence\n",
    "                if classification['label'] != 'normal' and classification['confidence'] > 0.5:\n",
    "                    # Extract locations\n",
    "                    locations = self.extract_locations(clean_text)\n",
    "                    \n",
    "                    if locations:\n",
    "                        for location in locations:\n",
    "                            coords = self.geocode_location(location)\n",
    "                            if coords:\n",
    "                                alert = {\n",
    "                                    'timestamp': tweet['created_at'] if isinstance(tweet['created_at'], datetime) else datetime.now(),\n",
    "                                    'latitude': coords[0],\n",
    "                                    'longitude': coords[1],\n",
    "                                    'disruption_type': classification['label'],\n",
    "                                    'confidence': classification['confidence'],\n",
    "                                    'location_name': location,\n",
    "                                    'description': clean_text[:100] + \"...\" if len(clean_text) > 100 else clean_text,\n",
    "                                    'tweet_id': tweet['id'],\n",
    "                                    'source': 'twitter_nlp'\n",
    "                                }\n",
    "                                alerts.append(alert)\n",
    "                    else:\n",
    "                        # If no specific location found, use general Chennai coordinates\n",
    "                        alert = {\n",
    "                            'timestamp': tweet['created_at'] if isinstance(tweet['created_at'], datetime) else datetime.now(),\n",
    "                            'latitude': 13.0827,\n",
    "                            'longitude': 80.2707,\n",
    "                            'disruption_type': classification['label'],\n",
    "                            'confidence': classification['confidence'],\n",
    "                            'location_name': 'Chennai',\n",
    "                            'description': clean_text[:100] + \"...\" if len(clean_text) > 100 else clean_text,\n",
    "                            'tweet_id': tweet['id'],\n",
    "                            'source': 'twitter_nlp'\n",
    "                        }\n",
    "                        alerts.append(alert)\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error analyzing tweet {tweet['id']}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        logger.info(f\"Generated {len(alerts)} disruption alerts from tweets\")\n",
    "        return alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd8a5ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_periodic_analysis(self, interval_minutes: int = 5):\n",
    "        \"\"\"Run tweet analysis every 5 minutes\"\"\"\n",
    "        logger.info(f\"Starting periodic tweet analysis every {interval_minutes} minutes\")\n",
    "        \n",
    "        def analyze_job():\n",
    "            try:\n",
    "                alerts = self.analyze_tweets()\n",
    "                \n",
    "                # Save alerts to file\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                filename = f\"twitter_alerts_{timestamp}.json\"\n",
    "                \n",
    "                with open(filename, 'w') as f:\n",
    "                    json.dump(alerts, f, indent=2, default=str)\n",
    "                \n",
    "                logger.info(f\"Saved {len(alerts)} alerts to {filename}\")\n",
    "                \n",
    "                # Print summary\n",
    "                print(f\"\\n=== Twitter NLP Analysis Results ===\")\n",
    "                print(f\"Timestamp: {datetime.now()}\")\n",
    "                print(f\"Total alerts: {len(alerts)}\")\n",
    "                \n",
    "                for alert in alerts[:5]:  # Show first 5 alerts\n",
    "                    print(f\"\\nAlert:\")\n",
    "                    print(f\"  Location: {alert['location_name']} ({alert['latitude']:.4f}, {alert['longitude']:.4f})\")\n",
    "                    print(f\"  Type: {alert['disruption_type']}\")\n",
    "                    print(f\"  Confidence: {alert['confidence']:.2f}\")\n",
    "                    print(f\"  Description: {alert['description']}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in periodic analysis: {e}\")\n",
    "        \n",
    "        # Schedule the job\n",
    "        schedule.every(interval_minutes).minutes.do(analyze_job)\n",
    "        \n",
    "        # Run once immediately\n",
    "        analyze_job()\n",
    "        \n",
    "        # Keep running\n",
    "        while True:\n",
    "            schedule.run_pending()\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73a368b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error running Twitter NLP model: 'TwitterNLPModel' object has no attribute 'analyze_tweets'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize the model\n",
    "    # Note: You'll need to provide your Twitter Bearer Token\n",
    "    TWITTER_BEARER_TOKEN = \"your_twitter_bearer_token_here\"\n",
    "    GOOGLE_MAPS_API_KEY = \"your_google_maps_api_key_here\"  # Optional\n",
    "    \n",
    "    # For demo purposes, we'll use mock data\n",
    "    try:\n",
    "        model = TwitterNLPModel(TWITTER_BEARER_TOKEN, GOOGLE_MAPS_API_KEY)\n",
    "        \n",
    "        # Run one-time analysis\n",
    "        alerts = model.analyze_tweets()\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\n=== Twitter NLP Model Results ===\")\n",
    "        print(f\"Total alerts generated: {len(alerts)}\")\n",
    "        \n",
    "        for alert in alerts:\n",
    "            print(f\"\\nAlert:\")\n",
    "            print(f\"  Location: {alert['location_name']} ({alert['latitude']:.4f}, {alert['longitude']:.4f})\")\n",
    "            print(f\"  Type: {alert['disruption_type']}\")\n",
    "            print(f\"  Confidence: {alert['confidence']:.2f}\")\n",
    "            print(f\"  Description: {alert['description']}\")\n",
    "            print(f\"  Timestamp: {alert['timestamp']}\")\n",
    "        \n",
    "        # Uncomment to run periodic analysis\n",
    "        # model.run_periodic_analysis(interval_minutes=5)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error running Twitter NLP model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0bc153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a78fa097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from transformers import pipeline\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e23dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76459e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterNLPModel:\n",
    "    def __init__(self):\n",
    "        # Initialize zero-shot classification model\n",
    "        self.classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "        self.disruption_labels = [\"protest\", \"accident\", \"road closure\", \"traffic jam\", \"construction\", \"emergency\", \"normal\"]\n",
    "\n",
    "        # Predefined Chennai locations\n",
    "        self.mock_locations = {\n",
    "            'nungambakkam': (13.0569, 80.2412),\n",
    "            'anna nagar': (13.0850, 80.2101),\n",
    "            't nagar': (13.0418, 80.2341),\n",
    "            'adyar': (13.0067, 80.2206),\n",
    "            'velachery': (12.9816, 80.2209),\n",
    "            'tambaram': (12.9249, 80.1000),\n",
    "            'chrompet': (12.9516, 80.1462),\n",
    "            'omr': (12.8956, 80.2267),\n",
    "            'ecr': (12.8270, 80.2420),\n",
    "            'mount road': (13.0569, 80.2412),\n",
    "            'velachery junction': (12.9800, 80.2210)\n",
    "        }\n",
    "\n",
    "    def preprocess_tweet(self, text: str) -> str:\n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "        text = re.sub(r\"@\\w+\", '', text)\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def classify_event(self, text: str) -> Dict:\n",
    "        try:\n",
    "            result = self.classifier(text, self.disruption_labels)\n",
    "            return {\n",
    "                \"label\": result[\"labels\"][0],\n",
    "                \"confidence\": result[\"scores\"][0],\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Classification error: {e}\")\n",
    "            return {\"label\": \"normal\", \"confidence\": 0.0}\n",
    "\n",
    "    def extract_location(self, text: str) -> Optional[Tuple[str, Tuple[float, float]]]:\n",
    "        text = text.lower()\n",
    "        for loc in self.mock_locations:\n",
    "            if loc in text:\n",
    "                return loc.title(), self.mock_locations[loc]\n",
    "        return None\n",
    "\n",
    "    def get_mock_tweets(self) -> List[Dict]:\n",
    "        return [\n",
    "            {\n",
    "                'id': '1',\n",
    "                'text': 'Heavy traffic jam at Nungambakkam area due to accident. Avoid the route!',\n",
    "                'created_at': datetime.now()\n",
    "            },\n",
    "            {\n",
    "                'id': '2',\n",
    "                'text': 'Protest happening near Anna Nagar causing road closures #ChennaiTraffic',\n",
    "                'created_at': datetime.now() - timedelta(minutes=15)\n",
    "            },\n",
    "            {\n",
    "                'id': '3',\n",
    "                'text': 'Construction work on OMR causing major delays. Take alternate route',\n",
    "                'created_at': datetime.now() - timedelta(minutes=30)\n",
    "            },\n",
    "            {\n",
    "                'id': '4',\n",
    "                'text': 'Emergency vehicles blocking Mount Road. Traffic moving slowly',\n",
    "                'created_at': datetime.now() - timedelta(minutes=45)\n",
    "            },\n",
    "            {\n",
    "                'id': '5',\n",
    "                'text': 'Water logging at Velachery junction after heavy rain. Roads flooded',\n",
    "                'created_at': datetime.now() - timedelta(minutes=60)\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    def analyze_tweets(self) -> List[Dict]:\n",
    "        tweets = self.get_mock_tweets()\n",
    "        alerts = []\n",
    "\n",
    "        for tweet in tweets:\n",
    "            clean_text = self.preprocess_tweet(tweet['text'])\n",
    "            classification = self.classify_event(clean_text)\n",
    "            if classification['label'] != 'normal' and classification['confidence'] > 0.5:\n",
    "                loc_result = self.extract_location(clean_text)\n",
    "                if loc_result:\n",
    "                    location_name, coords = loc_result\n",
    "                else:\n",
    "                    location_name = \"Chennai\"\n",
    "                    coords = (13.0827, 80.2707)\n",
    "\n",
    "                alerts.append({\n",
    "                    'timestamp': tweet['created_at'],\n",
    "                    'latitude': coords[0],\n",
    "                    'longitude': coords[1],\n",
    "                    'disruption_type': classification['label'],\n",
    "                    'confidence': classification['confidence'],\n",
    "                    'location_name': location_name,\n",
    "                    'description': clean_text,\n",
    "                    'tweet_id': tweet['id'],\n",
    "                })\n",
    "\n",
    "        return alerts\n",
    "\n",
    "    def render_map(self, alerts: List[Dict], filename: str = \"traffic_map.html\"):\n",
    "        base_map = folium.Map(location=[13.0827, 80.2707], zoom_start=11)\n",
    "\n",
    "        for alert in alerts:\n",
    "            folium.Marker(\n",
    "                location=[alert['latitude'], alert['longitude']],\n",
    "                popup=folium.Popup(\n",
    "                    f\"<b>{alert['disruption_type'].title()}</b><br>\"\n",
    "                    f\"<b>Location:</b> {alert['location_name']}<br>\"\n",
    "                    f\"<b>Confidence:</b> {alert['confidence']:.2f}<br>\"\n",
    "                    f\"<b>Description:</b> {alert['description']}\",\n",
    "                    max_width=300\n",
    "                ),\n",
    "                tooltip=alert['disruption_type'].title(),\n",
    "                icon=folium.Icon(color=\"red\" if alert['confidence'] > 0.7 else \"orange\")\n",
    "            ).add_to(base_map)\n",
    "\n",
    "        base_map.save(filename)\n",
    "        logger.info(f\"Map saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedf81a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Map saved to twitter_alerts_demo_map.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Twitter NLP Model Alerts ===\n",
      "\n",
      "Alert:\n",
      "  Location: Anna Nagar (13.0850, 80.2101)\n",
      "  Type: protest\n",
      "  Confidence: 0.71\n",
      "  Description: Protest happening near Anna Nagar causing road closures #ChennaiTraffic\n",
      "  Timestamp: 2025-07-11 14:30:52.395010\n",
      "\n",
      "Alert:\n",
      "  Location: Omr (12.8956, 80.2267)\n",
      "  Type: construction\n",
      "  Confidence: 0.77\n",
      "  Description: Construction work on OMR causing major delays. Take alternate route\n",
      "  Timestamp: 2025-07-11 14:15:52.395010\n",
      "\n",
      "Alert:\n",
      "  Location: Mount Road (13.0569, 80.2412)\n",
      "  Type: road closure\n",
      "  Confidence: 0.52\n",
      "  Description: Emergency vehicles blocking Mount Road. Traffic moving slowly\n",
      "  Timestamp: 2025-07-11 14:00:52.395010\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = TwitterNLPModel()\n",
    "    alerts = model.analyze_tweets()\n",
    "\n",
    "    print(\"\\n=== Twitter NLP Model Alerts ===\")\n",
    "    for alert in alerts:\n",
    "        print(f\"\\nAlert:\")\n",
    "        print(f\"  Location: {alert['location_name']} ({alert['latitude']:.4f}, {alert['longitude']:.4f})\")\n",
    "        print(f\"  Type: {alert['disruption_type']}\")\n",
    "        print(f\"  Confidence: {alert['confidence']:.2f}\")\n",
    "        print(f\"  Description: {alert['description']}\")\n",
    "        print(f\"  Timestamp: {alert['timestamp']}\")\n",
    "\n",
    "    model.render_map(alerts, filename=\"twitter_alerts_demo_map.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49964c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Map saved to twitter_alerts_demo_map.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Twitter NLP Model Alerts ===\n",
      "\n",
      "Alert:\n",
      "  Location: Anna Nagar (13.0850, 80.2101)\n",
      "  Type: protest\n",
      "  Confidence: 0.71\n",
      "  Description: Protest happening near Anna Nagar causing road closures #ChennaiTraffic\n",
      "  Timestamp: 2025-07-11 14:47:17.050082\n",
      "\n",
      "Alert:\n",
      "  Location: Omr (12.8956, 80.2267)\n",
      "  Type: construction\n",
      "  Confidence: 0.77\n",
      "  Description: Construction work on OMR causing major delays. Take alternate route\n",
      "  Timestamp: 2025-07-11 14:32:17.050082\n",
      "\n",
      "Alert:\n",
      "  Location: Mount Road (13.0569, 80.2412)\n",
      "  Type: road closure\n",
      "  Confidence: 0.52\n",
      "  Description: Emergency vehicles blocking Mount Road. Traffic moving slowly\n",
      "  Timestamp: 2025-07-11 14:17:17.050082\n",
      "\n",
      "✅ Alerts saved to 'twitter_alerts_output.json'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from transformers import pipeline\n",
    "import folium\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "class TwitterNLPModel:\n",
    "    def __init__(self):\n",
    "        # Initialize zero-shot classification model\n",
    "        self.classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "        self.disruption_labels = [\"protest\", \"accident\", \"road closure\", \"traffic jam\", \"construction\", \"emergency\", \"normal\"]\n",
    "\n",
    "        # Predefined Chennai locations\n",
    "        self.mock_locations = {\n",
    "            'nungambakkam': (13.0569, 80.2412),\n",
    "            'anna nagar': (13.0850, 80.2101),\n",
    "            't nagar': (13.0418, 80.2341),\n",
    "            'adyar': (13.0067, 80.2206),\n",
    "            'velachery': (12.9816, 80.2209),\n",
    "            'tambaram': (12.9249, 80.1000),\n",
    "            'chrompet': (12.9516, 80.1462),\n",
    "            'omr': (12.8956, 80.2267),\n",
    "            'ecr': (12.8270, 80.2420),\n",
    "            'mount road': (13.0569, 80.2412),\n",
    "            'velachery junction': (12.9800, 80.2210)\n",
    "        }\n",
    "\n",
    "    def preprocess_tweet(self, text: str) -> str:\n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "        text = re.sub(r\"@\\w+\", '', text)\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def classify_event(self, text: str) -> Dict:\n",
    "        try:\n",
    "            result = self.classifier(text, self.disruption_labels)\n",
    "            return {\n",
    "                \"label\": result[\"labels\"][0],\n",
    "                \"confidence\": result[\"scores\"][0],\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Classification error: {e}\")\n",
    "            return {\"label\": \"normal\", \"confidence\": 0.0}\n",
    "\n",
    "    def extract_location(self, text: str) -> Optional[Tuple[str, Tuple[float, float]]]:\n",
    "        text = text.lower()\n",
    "        for loc in self.mock_locations:\n",
    "            if loc in text:\n",
    "                return loc.title(), self.mock_locations[loc]\n",
    "        return None\n",
    "\n",
    "    def get_mock_tweets(self) -> List[Dict]:\n",
    "        return [\n",
    "            {\n",
    "                'id': '1',\n",
    "                'text': 'Heavy traffic jam at Nungambakkam area due to accident. Avoid the route!',\n",
    "                'created_at': datetime.now()\n",
    "            },\n",
    "            {\n",
    "                'id': '2',\n",
    "                'text': 'Protest happening near Anna Nagar causing road closures #ChennaiTraffic',\n",
    "                'created_at': datetime.now() - timedelta(minutes=15)\n",
    "            },\n",
    "            {\n",
    "                'id': '3',\n",
    "                'text': 'Construction work on OMR causing major delays. Take alternate route',\n",
    "                'created_at': datetime.now() - timedelta(minutes=30)\n",
    "            },\n",
    "            {\n",
    "                'id': '4',\n",
    "                'text': 'Emergency vehicles blocking Mount Road. Traffic moving slowly',\n",
    "                'created_at': datetime.now() - timedelta(minutes=45)\n",
    "            },\n",
    "            {\n",
    "                'id': '5',\n",
    "                'text': 'Water logging at Velachery junction after heavy rain. Roads flooded',\n",
    "                'created_at': datetime.now() - timedelta(minutes=60)\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    def analyze_tweets(self) -> List[Dict]:\n",
    "        tweets = self.get_mock_tweets()\n",
    "        alerts = []\n",
    "\n",
    "        for tweet in tweets:\n",
    "            clean_text = self.preprocess_tweet(tweet['text'])\n",
    "            classification = self.classify_event(clean_text)\n",
    "            if classification['label'] != 'normal' and classification['confidence'] > 0.5:\n",
    "                loc_result = self.extract_location(clean_text)\n",
    "                if loc_result:\n",
    "                    location_name, coords = loc_result\n",
    "                else:\n",
    "                    location_name = \"Chennai\"\n",
    "                    coords = (13.0827, 80.2707)\n",
    "\n",
    "                alerts.append({\n",
    "                    'timestamp': tweet['created_at'],\n",
    "                    'latitude': coords[0],\n",
    "                    'longitude': coords[1],\n",
    "                    'disruption_type': classification['label'],\n",
    "                    'confidence': classification['confidence'],\n",
    "                    'location_name': location_name,\n",
    "                    'description': clean_text,\n",
    "                    'tweet_id': tweet['id'],\n",
    "                })\n",
    "\n",
    "        return alerts\n",
    "\n",
    "    def render_map(self, alerts: List[Dict], filename: str = \"traffic_map.html\"):\n",
    "        base_map = folium.Map(location=[13.0827, 80.2707], zoom_start=11)\n",
    "\n",
    "        for alert in alerts:\n",
    "            folium.Marker(\n",
    "                location=[alert['latitude'], alert['longitude']],\n",
    "                popup=folium.Popup(\n",
    "                    f\"<b>{alert['disruption_type'].title()}</b><br>\"\n",
    "                    f\"<b>Location:</b> {alert['location_name']}<br>\"\n",
    "                    f\"<b>Confidence:</b> {alert['confidence']:.2f}<br>\"\n",
    "                    f\"<b>Description:</b> {alert['description']}\",\n",
    "                    max_width=300\n",
    "                ),\n",
    "                tooltip=alert['disruption_type'].title(),\n",
    "                icon=folium.Icon(color=\"red\" if alert['confidence'] > 0.7 else \"orange\")\n",
    "            ).add_to(base_map)\n",
    "\n",
    "        base_map.save(filename)\n",
    "        logger.info(f\"Map saved to {filename}\")\n",
    "import json\n",
    "def save_alerts_to_json(alerts: List[Dict], filename: str = \"twitter_alerts_output.json\"):\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(alerts, f, indent=2, default=str)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = TwitterNLPModel()\n",
    "    alerts = model.analyze_tweets()\n",
    "\n",
    "    print(\"\\n=== Twitter NLP Model Alerts ===\")\n",
    "    for alert in alerts:\n",
    "        print(f\"\\nAlert:\")\n",
    "        print(f\"  Location: {alert['location_name']} ({alert['latitude']:.4f}, {alert['longitude']:.4f})\")\n",
    "        print(f\"  Type: {alert['disruption_type']}\")\n",
    "        print(f\"  Confidence: {alert['confidence']:.2f}\")\n",
    "        print(f\"  Description: {alert['description']}\")\n",
    "        print(f\"  Timestamp: {alert['timestamp']}\")\n",
    "\n",
    "    # Render the map\n",
    "    model.render_map(alerts, filename=\"twitter_alerts_demo_map.html\")\n",
    "\n",
    "    save_alerts_to_json(alerts)\n",
    "\n",
    "    # ✅ Optional: return alerts if running in a larger application\n",
    "    # Just print to console for now, since __main__ doesn’t return\n",
    "    print(\"\\n✅ Alerts saved to 'twitter_alerts_output.json'\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ad8808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
